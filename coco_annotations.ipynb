{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-23T02:44:17.731535Z",
     "end_time": "2023-04-23T02:44:19.509407Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Erik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "image_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406],\n",
    "              std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T02:44:20.571480Z",
     "end_time": "2023-04-23T02:44:20.598502Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class CaptionPreprocessor:\n",
    "    def __init__(self, captions, vocab_threshold=5, max_caption_length=20):\n",
    "        self.max_caption_length = max_caption_length\n",
    "\n",
    "        all_tokens = [token for caption in captions for token in nltk.tokenize.word_tokenize(caption.lower())]\n",
    "        counter = Counter(all_tokens)\n",
    "        self.vocab = {token: idx for idx, (token, count) in enumerate(counter.items()) if count >= vocab_threshold}\n",
    "\n",
    "        self.vocab['<pad>'] = len(self.vocab)\n",
    "        self.vocab['<start>'] = len(self.vocab)\n",
    "        self.vocab['<end>'] = len(self.vocab)\n",
    "        self.vocab['<unk>'] = len(self.vocab)\n",
    "\n",
    "        self.idx_to_token = {idx: token for token, idx in self.vocab.items()}\n",
    "\n",
    "    def preprocess(self, caption):\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        caption_indices = [self.vocab['<start>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in tokens] + [self.vocab['<end>']]\n",
    "\n",
    "        if len(caption_indices) < self.max_caption_length:\n",
    "            caption_indices += [self.vocab['<pad>']] * (self.max_caption_length - len(caption_indices))\n",
    "\n",
    "        return caption_indices[:self.max_caption_length]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T02:44:21.652105Z",
     "end_time": "2023-04-23T02:44:21.671683Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class CustomCocoDataset(Dataset):\n",
    "    def __init__(self, coco_dataset, caption_preprocessor):\n",
    "        self.coco_dataset = coco_dataset\n",
    "        self.caption_preprocessor = caption_preprocessor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, caption_list = self.coco_dataset[idx]\n",
    "        caption = caption_list[0]\n",
    "        preprocessed_caption = torch.tensor(self.caption_preprocessor.preprocess(caption))\n",
    "        return img, preprocessed_caption"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T02:44:22.361493Z",
     "end_time": "2023-04-23T02:44:22.391704Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, num_layers, num_heads, mlp_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(patch_size, in_channels, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, (224 // patch_size) * (224 // patch_size) + 1, embed_dim))\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embed_dim, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.classification_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.positional_encoding[:, :-1]\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T02:45:03.511368Z",
     "end_time": "2023-04-23T02:45:03.527596Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.encoding = nn.Parameter(torch.zeros(1, max_len, d_model), requires_grad=False)\n",
    "\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, :, 0::2] = torch.sin(pos * div_term)\n",
    "        self.encoding[:, :, 1::2] = torch.cos(pos * div_term)\n",
    "\n",
    "\n",
    "class TransformerCaptionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, mlp_dim, max_len=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, captions, memory):\n",
    "        captions = self.embedding(captions) + self.positional_encoding.encoding[:, :captions.shape[1]]\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            captions = layer(captions, memory)\n",
    "\n",
    "        logits = self.output_layer(captions)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T02:45:04.570231Z",
     "end_time": "2023-04-23T02:45:04.589117Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, image_encoder, caption_decoder, embedding_size):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.caption_decoder = caption_decoder\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        image_features = self.image_encoder(images)\n",
    "        num_patches = (224 // 16) * (224 // 16)\n",
    "        image_features_flattened = image_features.permute(1, 0, 2).view(num_patches, -1, self.embedding_size)\n",
    "\n",
    "        start_token_embeddings = self.caption_decoder.embedding(torch.tensor([self.caption_decoder.embedding.num_embeddings - 2], device=device)).repeat(image_features.shape[0], 1, 1) # Get the <start> token embedding and repeat it for the batch size\n",
    "        memory = torch.cat([start_token_embeddings, image_features_flattened.permute(1, 0, 2)], dim=1) # Concatenate the start token embeddings with the flattened image features\n",
    "        output = self.caption_decoder(captions, memory)\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T02:45:05.724456Z",
     "end_time": "2023-04-23T02:45:05.762026Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = CocoCaptions(root='./coco/images',\n",
    "                       annFile='./coco/annotations/captions_train2014.json',\n",
    "                       transform=image_transform)\n",
    "captions = [entry['caption'] for entry in dataset.coco.anns.values()]\n",
    "caption_preprocessor = CaptionPreprocessor(captions)\n",
    "custom_dataset = CustomCocoDataset(dataset, caption_preprocessor)\n",
    "data_loader = DataLoader(custom_dataset, batch_size=32, shuffle=True, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "image_encoder = VisionTransformer(in_channels=3,\n",
    "                                  patch_size=16,\n",
    "                                  embed_dim=768,\n",
    "                                  num_layers=12,\n",
    "                                  num_heads=12,\n",
    "                                  mlp_dim=3072,\n",
    "                                  num_classes=768).to(device)\n",
    "caption_decoder = TransformerCaptionDecoder(vocab_size=len(caption_preprocessor.vocab),\n",
    "                                            d_model=768,\n",
    "                                            num_layers=6,\n",
    "                                            num_heads=8,\n",
    "                                            mlp_dim=2048).to(device)\n",
    "embedding_size = 768\n",
    "model = ImageCaptioningModel(image_encoder, caption_decoder, embedding_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=caption_preprocessor.vocab['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (images, captions) in enumerate(data_loader):\n",
    "        images = images.to(device)\n",
    "        captions_input = captions[:, :-1].to(device)\n",
    "        captions_target = captions[:, 1:].to(device)\n",
    "\n",
    "        print(\"Captions shape:\", captions_input.shape)\n",
    "        print(\"Memory shape:\", images.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images, captions_input)\n",
    "        loss = criterion(output.view(-1, len(caption_preprocessor.vacab)), captions_target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Iteration: {i}, Loss: {loss.item()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
