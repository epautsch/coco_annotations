{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-23T11:48:29.750146Z",
     "end_time": "2023-04-23T11:48:32.035034Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "import torch.nn.init as init\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406],\n",
    "              std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T11:48:38.502859Z",
     "end_time": "2023-04-23T11:48:38.521857Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CaptionPreprocessor:\n",
    "    def __init__(self, captions, vocab_threshold=5, max_caption_length=20):\n",
    "        self.max_caption_length = max_caption_length\n",
    "\n",
    "        all_tokens = [token for caption in captions for token in nltk.tokenize.word_tokenize(caption.lower())]\n",
    "        counter = Counter(all_tokens)\n",
    "        self.vocab = {token: idx for idx, (token, count) in enumerate(counter.items()) if count >= vocab_threshold}\n",
    "\n",
    "        self.vocab['<pad>'] = len(self.vocab)\n",
    "        self.vocab['<start>'] = len(self.vocab)\n",
    "        self.vocab['<end>'] = len(self.vocab)\n",
    "        self.vocab['<unk>'] = len(self.vocab)\n",
    "\n",
    "        self.idx_to_token = {idx: token for token, idx in self.vocab.items()}\n",
    "\n",
    "        self.captions_tokenized = self.tokenize_captions(captions)\n",
    "\n",
    "    def preprocess(self, caption):\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        caption_indices = [self.vocab['<start>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in tokens] + [self.vocab['<end>']]\n",
    "\n",
    "        if len(caption_indices) < self.max_caption_length:\n",
    "            caption_indices += [self.vocab['<pad>']] * (self.max_caption_length - len(caption_indices))\n",
    "\n",
    "        return caption_indices[:self.max_caption_length]\n",
    "\n",
    "    def tokenize_captions(self, captions):\n",
    "        return [self.preprocess(caption) for caption in captions]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T11:48:39.571436Z",
     "end_time": "2023-04-23T11:48:39.617453Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomCocoDataset(Dataset):\n",
    "    def __init__(self, coco_dataset, caption_preprocessor):\n",
    "        self.coco_dataset = coco_dataset\n",
    "        self.caption_preprocessor = caption_preprocessor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, caption_list = self.coco_dataset[idx]\n",
    "        caption = caption_list[0]\n",
    "        preprocessed_caption = torch.tensor(self.caption_preprocessor.preprocess(caption))\n",
    "        return img, preprocessed_caption"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T11:48:40.284194Z",
     "end_time": "2023-04-23T11:48:40.320193Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        init.xavier_uniform_(self.proj.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, num_layers, num_heads, mlp_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(patch_size, in_channels, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, (224 // patch_size) * (224 // patch_size) + 1, embed_dim))\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embed_dim, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.classification_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.positional_encoding[:, :-1]\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T11:48:41.159050Z",
     "end_time": "2023-04-23T11:48:41.191320Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.encoding = nn.Parameter(torch.zeros(1, max_len, d_model), requires_grad=False)\n",
    "\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, :, 0::2] = torch.sin(pos * div_term)\n",
    "        self.encoding[:, :, 1::2] = torch.cos(pos * div_term)\n",
    "\n",
    "\n",
    "class TransformerCaptionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, mlp_dim, max_len=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        init.xavier_uniform_(self.embedding.weight)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, captions, memory):\n",
    "        captions = self.embedding(captions) + self.positional_encoding.encoding[:, :captions.shape[1]]\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            captions = layer(captions, memory)\n",
    "\n",
    "        logits = self.output_layer(captions)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T11:48:43.019288Z",
     "end_time": "2023-04-23T11:48:43.058279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, image_encoder, caption_decoder, embedding_size, start_token_index):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.caption_decoder = caption_decoder\n",
    "        self.embedding_size = embedding_size\n",
    "        self.start_token_index = start_token_index\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        image_features = self.image_encoder(images)\n",
    "        num_patches = (224 // 16) * (224 // 16)\n",
    "        image_features_flattened = image_features.permute(1, 0, 2).reshape(-1, num_patches, self.embedding_size)\n",
    "\n",
    "        # print(\"Start token index:\", caption_preprocessor.vocab['<start>'])\n",
    "\n",
    "        start_token_embeddings = self.caption_decoder.embedding(torch.tensor([self.start_token_index], device=images.device)).repeat(image_features.shape[0], 1, 1) # Get the <start> token embedding and repeat it for the batch size\n",
    "        image_features_summed = image_features_flattened.sum(dim=1).unsqueeze(1)\n",
    "        memory = torch.cat([start_token_embeddings, image_features_summed], dim=1) # Concatenate the start token embeddings with the flattened image features\n",
    "\n",
    "        memory = memory.transpose(0, 1)\n",
    "\n",
    "        captions = captions.transpose(0, 1)\n",
    "\n",
    "        output = self.caption_decoder(captions, memory)\n",
    "\n",
    "        output = output.transpose(0, 1)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T11:48:43.915552Z",
     "end_time": "2023-04-23T11:48:43.940584Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self.learning_rate()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            if param_group['lr'] != lr:\n",
    "                print(f\"Learning rate changed: {param_group['lr']} -> {lr}\")\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def learning_rate(self):\n",
    "        arg1 = self.current_step ** -0.5\n",
    "        arg2 = min(self.current_step * self.warmup_steps ** -1.5, 1)\n",
    "        return (self.d_model ** -0.5) * min(arg1, arg2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T11:48:44.687740Z",
     "end_time": "2023-04-23T11:48:44.730246Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_and_save(train_losses, val_losses, learning_rates, max_min_loss_diffs):\n",
    "    plt.style.use('classic')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(train_losses, label='Train Loss')\n",
    "    ax.plot(val_losses, label='Validation Loss')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Loss', fontsize=14)\n",
    "    ax.set_title('Training and Validation Losses', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('losses.png')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(learning_rates, label='Learning Rate')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Learning Rate', fontsize=14)\n",
    "    ax.set_title('Learning Rate Schedule', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('learning_rates.png')\n",
    "\n",
    "    fig_ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(max_min_loss_diffs, label='Loss Difference')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Loss Difference', fontsize=14)\n",
    "    ax.set_title('Difference Between Max and Min Loss per Epoch', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('loss_differences.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T11:48:46.014614Z",
     "end_time": "2023-04-23T11:48:46.035802Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = CocoCaptions(root='./coco/images',\n",
    "                       annFile='./coco/annotations/captions_train2014.json',\n",
    "                       transform=image_transform)\n",
    "val_dataset = CocoCaptions(root='./coco/images',\n",
    "                           annFile='./coco/annotations/captions_val2014.json',\n",
    "                           transform=image_transform)\n",
    "train_captions = [entry['caption'] for entry in train_dataset.coco.anns.values()]\n",
    "val_captions = [entry['caption'] for entry in val_dataset.coco.anns.values()]\n",
    "\n",
    "max_caption_length_train = max([len(nltk.tokenize.word_tokenize(caption.lower())) for caption in train_captions])\n",
    "max_caption_length_val = max([len(nltk.tokenize.word_tokenize(caption.lower())) for caption in val_captions])\n",
    "max_caption_length = max(max_caption_length_train, max_caption_length_val)\n",
    "print('Maximum caption length (without <start>, <end>, and <pad> tokens):', max_caption_length)\n",
    "\n",
    "caption_preprocessor = CaptionPreprocessor(train_captions + val_captions)\n",
    "custom_train_dataset = CustomCocoDataset(train_dataset, caption_preprocessor)\n",
    "custom_val_dataset = CustomCocoDataset(val_dataset, caption_preprocessor)\n",
    "\n",
    "batch_size = 64\n",
    "train_data_loader = DataLoader(custom_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_data_loader = DataLoader(custom_val_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T11:48:48.148264Z",
     "end_time": "2023-04-23T11:51:39.281544Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "image_encoder = VisionTransformer(in_channels=3,\n",
    "                                  patch_size=16,\n",
    "                                  embed_dim=1600,\n",
    "                                  num_layers=16,\n",
    "                                  num_heads=16,\n",
    "                                  mlp_dim=5120,\n",
    "                                  num_classes=1600).to(device)\n",
    "max_caption_index = max([max(caption) for caption in caption_preprocessor.captions_tokenized])\n",
    "caption_decoder = TransformerCaptionDecoder(vocab_size=max_caption_index + 1,\n",
    "                                            d_model=1600,\n",
    "                                            num_layers=20,\n",
    "                                            num_heads=20,\n",
    "                                            mlp_dim=5120).to(device)\n",
    "embedding_size = 1600\n",
    "model = ImageCaptioningModel(image_encoder, caption_decoder, embedding_size, caption_preprocessor.vocab['<start>']).to(device)\n",
    "\n",
    "useTwoGPUs = True\n",
    "if torch.cuda.device_count() > 1 and useTwoGPUs:\n",
    "    print(f'Using {torch.cuda.device_count()} GPUs')\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "total_samples = len(train_data_loader.dataset)\n",
    "batch_size = train_data_loader.batch_size\n",
    "max_iterations = math.ceil(total_samples / batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=caption_preprocessor.vocab['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "# scheduler = NoamScheduler(optimizer, d_model=1600, warmup_steps=4000)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=max_iterations * 2, eta_min=1e-6)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "learning_rates = []\n",
    "max_min_loss_diffs = []\n",
    "save_name = ''\n",
    "\n",
    "print('**********STARTING TRAINING**********')\n",
    "training_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    epoch_max_loss = float('-inf')\n",
    "    epoch_min_loss = float('inf')\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    print(f'Total samples: {total_samples}, Batch size: {batch_size}, Maximum iterations: {max_iterations}')\n",
    "\n",
    "    epoch_train_start = time.time()\n",
    "    for i, (images, captions) in enumerate(train_data_loader):\n",
    "        images = images.to(device)\n",
    "        captions_input = captions[:, :-1].to(device)\n",
    "        captions_target = captions[:, 1:].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images, captions_input)\n",
    "\n",
    "        loss = criterion(output.reshape(-1, 28796), captions_target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if loss.item() > epoch_max_loss:\n",
    "            epoch_max_loss = loss.item()\n",
    "            print(f'Max loss set to: {epoch_max_loss}')\n",
    "        if loss.item() < epoch_min_loss:\n",
    "            epoch_min_loss = loss.item()\n",
    "            print(f'Min loss set to: {epoch_min_loss}')\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Iteration: {i}, Loss: {loss.item()}')\n",
    "\n",
    "    epoch_train_end = time.time()\n",
    "    epoch_train_time = epoch_train_end - epoch_train_start\n",
    "    print(f'Epoch {epoch+1} training time: {epoch_train_time}')\n",
    "\n",
    "    epoch_max_min_diff = epoch_max_loss - epoch_min_loss\n",
    "    if epoch + 1 != 1:\n",
    "        max_min_loss_diffs.append(epoch_max_min_diff)\n",
    "    print(f'Difference between max and min loss in epoch {epoch+1}: {epoch_max_min_diff}')\n",
    "\n",
    "    train_loss /= len(train_data_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    epoch_val_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for images, captions in val_data_loader:\n",
    "            images = images.to(device)\n",
    "            captions_input = captions[:, :-1].to(device)\n",
    "            captions_target = captions[:, 1:].to(device)\n",
    "\n",
    "            output = model(images, captions_input)\n",
    "            loss = criterion(output.reshape(-1, 28796), captions_target.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    epoch_val_end = time.time()\n",
    "    epoch_val_time = epoch_val_end - epoch_val_start\n",
    "    print(f'Epoch {epoch+1} validation time: {epoch_val_time}')\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    epoch_time = epoch_end - epoch_start\n",
    "    print(f'Epoch {epoch+1} total time: {epoch_time}')\n",
    "\n",
    "    val_loss /= len(val_data_loader)\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "        if os.path.exists(save_name):\n",
    "            os.remove(save_name)\n",
    "\n",
    "        save_name = f'best_loss_model_{epoch}.pth'\n",
    "        torch.save(model.state_dict(), save_name)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "training_end = time.time()\n",
    "training_time = training_end - training_start\n",
    "print(f'Total training time: {training_time}')\n",
    "\n",
    "plot_and_save(train_losses, val_losses, learning_rates, max_min_loss_diffs)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
