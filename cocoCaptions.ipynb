{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406],\n",
    "              std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CaptionPreprocessor:\n",
    "    def __init__(self, captions, tokenizer, max_caption_length=14):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_caption_length = max_caption_length\n",
    "        self.total_caption_length = 0\n",
    "        self.total_squared_caption_length = 0\n",
    "\n",
    "        if not self.tokenizer.bos_token:\n",
    "            self.tokenizer.bos_token = '[CLS]'\n",
    "        if not self.tokenizer.eos_token:\n",
    "            self.tokenizer.eos_token = '[SEP]'\n",
    "\n",
    "        self.captions_tokenized = self.tokenize_captions(captions)\n",
    "\n",
    "    def preprocess(self, caption):\n",
    "        tokens = [self.tokenizer.bos_token] + self.tokenizer.tokenize(caption) + [self.tokenizer.eos_token]\n",
    "        caption_indices = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        caption_length = len(caption_indices)\n",
    "\n",
    "        if caption_length < self.max_caption_length:\n",
    "            caption_indices += [self.tokenizer.pad_token_id] * (self.max_caption_length - caption_length)\n",
    "\n",
    "        self.total_caption_length += caption_length\n",
    "        self.total_squared_caption_length += caption_length**2\n",
    "\n",
    "        return caption_indices[:self.max_caption_length]\n",
    "\n",
    "    def tokenize_captions(self, captions):\n",
    "        return [self.preprocess(caption) for caption in captions]\n",
    "\n",
    "    def get_average_caption_length(self):\n",
    "        return self.total_caption_length / len(self.captions_tokenized)\n",
    "\n",
    "    def get_caption_length_standard_deviation(self):\n",
    "        mean = self.get_average_caption_length()\n",
    "        variance = (self.total_squared_caption_length / len(self.captions_tokenized)) - mean**2\n",
    "        std_dev = math.sqrt(max(0, variance))\n",
    "        return std_dev"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomCocoDataset(Dataset):\n",
    "    def __init__(self, coco_dataset, caption_preprocessor, num_captions=5):\n",
    "        self.coco_dataset = coco_dataset\n",
    "        self.caption_preprocessor = caption_preprocessor\n",
    "        self.num_captions = num_captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, caption_list = self.coco_dataset[idx]\n",
    "\n",
    "        # hits when using test dataset which doesn't have captions\n",
    "        if not caption_list:\n",
    "            return img, None\n",
    "\n",
    "        selected_caption = random.choice(caption_list[:self.num_captions])\n",
    "        preprocessed_caption = torch.tensor(self.caption_preprocessor.preprocess(selected_caption))\n",
    "        return img, preprocessed_caption"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        init.xavier_uniform_(self.proj.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, num_layers, num_heads, mlp_dim, num_classes, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(patch_size, in_channels, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, (224 // patch_size) * (224 // patch_size) + 1, embed_dim))\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.classification_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.positional_encoding[:, :-1]\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_len = x.size(1)\n",
    "        encoding = torch.zeros(1, max_len, self.d_model, device=x.device, requires_grad=False)\n",
    "\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float, device=x.device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, device=x.device).float() * (-torch.log(torch.tensor(10000.0, device=x.device)) / self.d_model))\n",
    "        encoding[:, :, 0::2] = torch.sin(pos * div_term)\n",
    "        encoding[:, :, 1::2] = torch.cos(pos * div_term)\n",
    "\n",
    "        x = x.add(encoding)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerCaptionDecoder(nn.Module):\n",
    "    def __init__(self, auto_model, d_model, num_layers, num_heads, mlp_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.auto_model = auto_model\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model, num_heads, mlp_dim, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(d_model, self.auto_model.config.vocab_size)\n",
    "        init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, captions, memory):\n",
    "        captions = self.auto_model.get_input_embeddings()(captions)\n",
    "        captions = self.positional_encoding(captions).detach()\n",
    "\n",
    "        memory = memory.transpose(0, 1)\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            captions = layer(captions, memory[:, :captions.size(1), :])\n",
    "\n",
    "        logits = self.output_layer(captions)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, image_encoder, caption_decoder):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.caption_decoder = caption_decoder\n",
    "        self.start_token_index = caption_decoder.auto_model.config.bos_token_id or 0\n",
    "        self.embedding_size = caption_decoder.auto_model.config.hidden_size\n",
    "        self.image_feature_linear = nn.Linear(768, self.embedding_size)\n",
    "\n",
    "    def forward(self, images, captions, teacher_forcing=True):\n",
    "        image_features = self.image_encoder(images)\n",
    "        num_patches = (224 // 16) * (224 // 16)\n",
    "        # image_features_flattened = image_features.permute(1, 0, 2).reshape(-1, num_patches, self.embedding_size)\n",
    "\n",
    "        start_token_tensor = torch.tensor([self.start_token_index], dtype=torch.long, device=images.device)\n",
    "        start_token_embeddings = self.caption_decoder.auto_model.embeddings(start_token_tensor).repeat(image_features.shape[0], 1, 1) # getting start token embedding and repeating it for batch size\n",
    "        image_features_summed = image_features.sum(dim=1).unsqueeze(1)\n",
    "        image_features_summed = self.image_feature_linear(image_features_summed)\n",
    "        memory = torch.cat([start_token_embeddings, image_features_summed], dim=1) # Concat the start token embeddings with the flattened image features\n",
    "        memory = memory.transpose(0, 1)\n",
    "\n",
    "        if teacher_forcing:\n",
    "            captions_input = captions[:, :-1].to(device)\n",
    "            captions_output = self.caption_decoder(captions_input, memory)\n",
    "        else:\n",
    "            captions_output = torch.zeros_like(captions).to(device)\n",
    "            logits_output = torch.zeros((captions.size(0), captions.size(1), self.caption_decoder.auto_model.config.vocab_size), device=device)\n",
    "            captions_output[:, 0] = start_token_tensor\n",
    "            for t in range(1, captions.size(1)):\n",
    "                captions_input = captions_output[:, :t].to(device)\n",
    "                logits = self.caption_decoder(captions_input, memory[:, :t].clone())\n",
    "                logits_output[:, t] = logits[:, -1]\n",
    "                captions_output[:, t] = logits[:, -1].argmax(-1)\n",
    "\n",
    "            captions_output = logits_output\n",
    "\n",
    "        return captions_output\n",
    "\n",
    "\n",
    "    def update_dropout_rate(self, new_dropout_rate):\n",
    "        def update_dropout(module):\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.p = new_dropout_rate\n",
    "\n",
    "        self.caption_decoder.apply(update_dropout)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def use_teacher_forcing(teacher_forcing_ratio):\n",
    "    return random.random() < float(teacher_forcing_ratio)\n",
    "\n",
    "\n",
    "def train_one_epoch(model,\n",
    "                    dataloader,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    scheduler,\n",
    "                    device,\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    avg_every,\n",
    "                    learning_rates,\n",
    "                    stepCounter=None,\n",
    "                    teacher_forcing_ratio=1.0):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    last_x_losses = []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "\n",
    "    for i, (images, captions) in enumerate(progress_bar):\n",
    "        images = images.to(device)\n",
    "        captions_input = captions.to(device)\n",
    "\n",
    "        use_tf = use_teacher_forcing(teacher_forcing_ratio)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(images, captions_input, teacher_forcing=use_tf)\n",
    "\n",
    "        if use_tf:\n",
    "            captions_target = captions[:, 1:-1].to(device)\n",
    "        else:\n",
    "            captions_target = captions[:, 1:].to(device)\n",
    "\n",
    "        loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), captions_target.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # step.addAstep() # use with other schedulers\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        last_x_losses.append(loss.item())\n",
    "\n",
    "        if i % avg_every == 0 and i != 0:\n",
    "            avg_loss = sum(last_x_losses) / len(last_x_losses)\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Iteration: {i}, Loss (last {avg_every} iterations): {avg_loss:.4f}')\n",
    "            # progress_bar.set_postfix(avg_loss_last_x_iterations=f'last {avg_every}: {avg_loss:.4f}')\n",
    "            last_x_losses = []\n",
    "\n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in tqdm(dataloader, desc='Validating'):\n",
    "            image = images.to(device)\n",
    "            captions_input = captions[:, :-1].to(device)\n",
    "            captions_target = captions[:, 1:].to(device)\n",
    "\n",
    "            output = model(images, captions_input, teacher_forcing=False)\n",
    "\n",
    "            captions_target = captions[:, 1:-1].to(device)\n",
    "            loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), captions_target.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(dataloader)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class stepCounter:\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "\n",
    "    def addAstep(self):\n",
    "        self.steps += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000, scaling_factor=1.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self.learning_rate()\n",
    "        # print line used for debugging optimizer state on reload\n",
    "        # print(f'NoamScheduler step: {self.current_step}, Learning rate: {lr}')\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def learning_rate(self):\n",
    "        arg1 = self.current_step ** -0.5\n",
    "        arg2 = min(self.current_step * self.warmup_steps ** -1.5, 1)\n",
    "        return (self.d_model ** -0.5) * min(arg1, arg2) * self.scaling_factor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TeacherForcingScheduler:\n",
    "    def __init__(self, max_epochs, initial_teacher_forcing_ratio=1.0, warmup_steps=4000):\n",
    "        self.curr_epoch = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.initial_teacher_forcing_ratio = initial_teacher_forcing_ratio\n",
    "        self.curr_teacher_forcing_ratio = initial_teacher_forcing_ratio\n",
    "        self.last_val_loss = float('inf')\n",
    "        self.epochs_since_best_val_loss_set = 0\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.tf_history = []\n",
    "\n",
    "\n",
    "    # Linear decay\n",
    "    def step(self, val_loss, curr_sched_step):\n",
    "        self.tf_history.append(self.curr_teacher_forcing_ratio)\n",
    "\n",
    "        if val_loss < self.last_val_loss:\n",
    "            self.epochs_since_best_val_loss_set = 0\n",
    "        elif curr_sched_step > self.warmup_steps:\n",
    "            self.epochs_since_best_val_loss_set += 1\n",
    "\n",
    "        if self.epochs_since_best_val_loss_set < 2:\n",
    "            self.linear_decay()\n",
    "\n",
    "        self.curr_epoch += 1\n",
    "\n",
    "\n",
    "    def linear_decay(self):\n",
    "        linear_decay_ratio = self.initial_teacher_forcing_ratio * (1 - self.curr_epoch / self.max_epochs)\n",
    "        self.curr_teacher_forcing_ratio = linear_decay_ratio\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_and_save(train_losses, val_losses, learning_rates):\n",
    "    plt.style.use('classic')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(train_losses, label='Train Loss')\n",
    "    ax.plot(val_losses, label='Validation Loss')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Loss', fontsize=14)\n",
    "    ax.set_title('Training and Validation Losses', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('losses.png')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(learning_rates, label='Learning Rate')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Learning Rate', fontsize=14)\n",
    "    ax.set_title('Learning Rate Schedule', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('learning_rates.png')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_lists_to_file(file_path, train_losses, val_losses, learning_rates):\n",
    "    data = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'learning_rates': learning_rates,\n",
    "    }\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def load_lists_from_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data['train_losses'], data['val_losses'], data['learning_rates']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Needed when running inference on laptop\n",
    "def adjust_state_dict_keys(state_dict):\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key.replace('module.', '')\n",
    "        new_state_dict[new_key] = value\n",
    "    return new_state_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "train_dataset = CocoCaptions(root='./coco/images',\n",
    "                       annFile='./coco/annotations/captions_train2014.json',\n",
    "                       transform=image_transform)\n",
    "val_dataset = CocoCaptions(root='./coco/images',\n",
    "                           annFile='./coco/annotations/captions_val2014.json',\n",
    "                           transform=image_transform)\n",
    "train_captions = [entry['caption'] for entry in train_dataset.coco.anns.values()]\n",
    "val_captions = [entry['caption'] for entry in val_dataset.coco.anns.values()]\n",
    "\n",
    "caption_preprocessor = CaptionPreprocessor(train_captions + val_captions, tokenizer)\n",
    "\n",
    "max_caption_length_train = max([len(caption.split()) for caption in train_captions])\n",
    "max_caption_length_val = max([len(caption.split()) for caption in val_captions])\n",
    "max_caption_length = max(max_caption_length_train, max_caption_length_val)\n",
    "\n",
    "print(max_caption_length_train, max_caption_length_val, max_caption_length)\n",
    "print('Maximum caption length (without <start>, <end>, and <pad> tokens):', max_caption_length)\n",
    "\n",
    "average_caption_length = caption_preprocessor.get_average_caption_length()\n",
    "std_dev_caption_length = caption_preprocessor.get_caption_length_standard_deviation()\n",
    "\n",
    "print('Average caption length:', average_caption_length)\n",
    "print('Standard deviation of caption length:', std_dev_caption_length)\n",
    "\n",
    "custom_train_dataset = CustomCocoDataset(train_dataset, caption_preprocessor, num_captions=5)\n",
    "custom_val_dataset = CustomCocoDataset(val_dataset, caption_preprocessor, num_captions=5)\n",
    "\n",
    "batch_size = 320\n",
    "# num_workers = os.cpu_count()\n",
    "num_workers = 16\n",
    "print('CPU COUNT:', num_workers)\n",
    "train_data_loader = DataLoader(custom_train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "val_data_loader = DataLoader(custom_val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def display_random_sample(dataset, tokenizer):\n",
    "    random_idx = np.random.randint(0, len(dataset))\n",
    "    image, caption = dataset[random_idx]\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    image = (image * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
    "    image = np.clip(image, 0, 1)\n",
    "\n",
    "    caption_text = tokenizer.decode(caption.numpy(), skip_special_tokens=False)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(caption_text)\n",
    "    plt.show()\n",
    "\n",
    "# Display a random sample from the training dataset\n",
    "# display_random_sample(custom_train_dataset, tokenizer)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "image_encoder = VisionTransformer(in_channels=3,\n",
    "                                  patch_size=16,\n",
    "                                  embed_dim=768,\n",
    "                                  num_layers=4,\n",
    "                                  num_heads=16,\n",
    "                                  mlp_dim=768,\n",
    "                                  num_classes=768).to(device)\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(tokenizer_name).to(device)\n",
    "caption_decoder = TransformerCaptionDecoder(auto_model=auto_model,\n",
    "                                            d_model=768,\n",
    "                                            num_layers=4,\n",
    "                                            num_heads=16,\n",
    "                                            mlp_dim=768,\n",
    "                                            dropout=0.2).to(device)\n",
    "\n",
    "model = ImageCaptioningModel(image_encoder, caption_decoder).to(device)\n",
    "\n",
    "useTwoGPUs = True\n",
    "if torch.cuda.device_count() > 1 and useTwoGPUs:\n",
    "    print(f'Using {torch.cuda.device_count()} GPUs')\n",
    "    model = nn.DataParallel(model)\n",
    "# start newwwwwwww\n",
    "num_epochs = 240\n",
    "\n",
    "total_samples = len(train_data_loader.dataset)\n",
    "batch_size = train_data_loader.batch_size\n",
    "max_iterations = math.ceil(total_samples / batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0,) # weight_decay=2e-6)\n",
    "\n",
    "warmup_steps = 1000\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.67, patience=2, verbose=True)\n",
    "scheduler = NoamScheduler(optimizer, d_model=768, warmup_steps=warmup_steps, scaling_factor=0.01)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs * max_iterations, eta_min=1e-6)\n",
    "# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=int((num_epochs * max_iterations) / 6.5), T_mult=2, eta_min=1e-6)\n",
    "\n",
    "# tf scheduler\n",
    "# tf_scheduler = TeacherForcingScheduler(num_epochs, warmup_steps=warmup_steps)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "val_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "load_best_model = True\n",
    "load_final = True\n",
    "best_model_path = 'larger_attempt_3_FINAL_221_230_tf_0_6.pt'\n",
    "save_lists_path = 'larger_attempt_3_FINAL_221_230_tf_0_6.pkl'\n",
    "if load_best_model and os.path.exists(best_model_path):\n",
    "    if torch.cuda.is_available():\n",
    "        checkpoint = torch.load(best_model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        checkpoint = torch.load(best_model_path, map_location=torch.device('cpu'))\n",
    "        adjusted_state_dict = adjust_state_dict_keys(checkpoint['model_state_dict'])\n",
    "        model.load_state_dict(adjusted_state_dict)\n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.optimizer = optimizer\n",
    "    scheduler.current_step = checkpoint['scheduler_state_dict']['current_step'] # use with noam\n",
    "    # scheduler.state_dict = checkpoint['scheduler_state_dict'] # use with other schedulers\n",
    "    # tf sched\n",
    "    # tf_scheduler = checkpoint['tf_scheduler_state_dict']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    train_losses, val_losses, learning_rates = load_lists_from_file(save_lists_path)\n",
    "    start_epoch = len(train_losses)\n",
    "    if start_epoch >= num_epochs:\n",
    "        raise ValueError('Number of epochs to train on is too small')\n",
    "    training_range = range(start_epoch, num_epochs)\n",
    "    print(len(train_losses))\n",
    "    print('Loaded best saved model...')\n",
    "    print(f'Validation loss of the loaded model: {best_val_loss:.4f}')\n",
    "else:\n",
    "    training_range = range(num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_dropout = True\n",
    "new_dropout_rate = 0.2\n",
    "if new_dropout:\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.update_dropout_rate(new_dropout_rate)\n",
    "    else:\n",
    "        model.update_dropout_rate(new_dropout_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# stepCounter = stepCounter() # use with other schedulers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('**********STARTING TRAINING**********')\n",
    "training_start = time.time()\n",
    "for epoch in training_range:\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    print(f'Total samples: {total_samples}, Batch size: {batch_size}, Maximum iterations: {max_iterations}')\n",
    "\n",
    "    avg_every = 50\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    # print(old_lr, stepCounter.steps) # use with other schedulers\n",
    "    print(old_lr, scheduler.current_step) # use with noam\n",
    "\n",
    "    # tf scheduler\n",
    "    # old_tf_ratio = tf_scheduler.curr_teacher_forcing_ratio\n",
    "    # tf_scheduler.step(val_loss, scheduler.current_step)\n",
    "\n",
    "    train_loss = train_one_epoch(model, train_data_loader, criterion, optimizer, scheduler, device, epoch, num_epochs, avg_every, learning_rates, teacher_forcing_ratio=0.9) # stepCounter) # use with other schedulers\n",
    "    # 1-100: 1.0\n",
    "    # 101-120: 0.9\n",
    "    # 121-160: 0.8\n",
    "    # 161-170: 0.9\n",
    "    # 171-220: 0.7\n",
    "    # 221-230: 0.6\n",
    "    # 231-240: 0.9; decoder dropout: 0.2\n",
    "    print(f'TRAINING LOSS FOR EPOCH {epoch + 1}: {train_loss:.4f}')\n",
    "\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr != old_lr:\n",
    "        print(f'****LR changed from {old_lr} ==> {new_lr}****')\n",
    "\n",
    "    val_loss = evaluate(model, val_data_loader, criterion, device)\n",
    "    print(f'CURRENT BEST VALIDATION LOSS: {best_val_loss:.4f}')\n",
    "    print(f'VALIDATION LOSS FOR EPOCH {epoch + 1}: {val_loss:.4f}')\n",
    "\n",
    "    # tf scheduler\n",
    "    # new_tf_ratio = tf_scheduler.curr_teacher_forcing_ratio\n",
    "    # if old_tf_ratio != new_tf_ratio:\n",
    "    #     print(f'****TF RATIO CHANGED FROM {old_tf_ratio} ==> {new_tf_ratio}****')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        if load_final:\n",
    "            save_name = 'larger_attempt_3.pt'\n",
    "            save_lists_path = 'larger_attempt_3.pkl'\n",
    "        else:\n",
    "            save_name = best_model_path\n",
    "\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.__dict__, # use with noam\n",
    "            # 'scheduler_state_dict': scheduler.state_dict(), # use with other schedulers\n",
    "            'best_val_loss': best_val_loss,\n",
    "            # 'tf_scheduler_state_dict': tf_scheduler.__dict__\n",
    "        }, save_name)\n",
    "        save_lists_to_file(save_lists_path, train_losses, val_losses, learning_rates)\n",
    "        print(f'********************NEW BEST MODEL SAVED @ VAL: {best_val_loss:.4f}********************')\n",
    "\n",
    "    if epoch == num_epochs - 1:\n",
    "        final_val_loss = best_val_loss\n",
    "        final_save_name = 'larger_attempt_3_FINAL_231_240_tf_0_9_dropOut_0_2.pt'\n",
    "        final_save_lists = 'larger_attempt_3_FINAL_231_240_tf_0_9_dropOut_0_2.pkl'\n",
    "\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.__dict__, # use with noam\n",
    "            # 'scheduler_state_dict': scheduler.state_dict(), # use with other schedulers\n",
    "            'best_val_loss': final_val_loss,\n",
    "            # 'tf_scheduler_state_dict': tf_scheduler.__dict__\n",
    "        }, final_save_name)\n",
    "        save_lists_to_file(final_save_lists, train_losses, val_losses, learning_rates)\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} total time: {epoch_end - epoch_start}')\n",
    "\n",
    "training_end = time.time()\n",
    "print(f'Total training time: {training_end - training_start}')\n",
    "\n",
    "\n",
    "\n",
    "plot_and_save(train_losses, val_losses, learning_rates)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "#\n",
    "# test_dataset = CocoCaptions(root='./coco/images',\n",
    "#                             annFile='./coco/annotations/image_info_test2014.json',\n",
    "#                             transform=image_transform)\n",
    "# custom_test_dataset = CustomCocoDataset(test_dataset, caption_preprocessor, num_captions=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def generate_caption(model, image, tokenizer, device, max_len=14):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         image = image.unsqueeze(0).to(device)\n",
    "#         image_features = model.image_encoder(image)\n",
    "#         start_token_tensor = torch.tensor([model.start_token_index], dtype=torch.long, device=device)\n",
    "#         start_token_embeddings = model.caption_decoder.auto_model.embeddings(start_token_tensor).repeat(image_features.shape[0], 1, 1)\n",
    "#         image_features_summed = image_features.sum(dim=1).unsqueeze(1)\n",
    "#         image_features_summed = model.image_feature_linear(image_features_summed)\n",
    "#         memory = torch.cat([start_token_embeddings, image_features_summed], dim=1)\n",
    "#         memory = memory.transpose(0, 1)\n",
    "#\n",
    "#         captions_output = torch.zeros((1, max_len)).long().to(device)\n",
    "#         captions_output[:, 0] = model.start_token_index\n",
    "#\n",
    "#         for t in range(1, max_len):\n",
    "#             captions_input = captions_output[:, :t].to(device)\n",
    "#             output = model.caption_decoder(captions_input, memory[:, :t].clone())\n",
    "#             captions_output[:, t] = output[:, -1].argmax(-1)\n",
    "#\n",
    "#         decoded_caption = tokenizer.decode(captions_output.squeeze().tolist(), skip_special_tokens=False)\n",
    "#\n",
    "#     return decoded_caption\n",
    "#\n",
    "#\n",
    "# def display_image(image, caption_text):\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(caption_text)\n",
    "#     plt.show()\n",
    "#\n",
    "#\n",
    "# inverse_transform = Compose([\n",
    "#     Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], std=[1/0.229, 1/0.224, 1/0.225])\n",
    "# ])\n",
    "#\n",
    "# random_idx = random.randint(0, len(custom_test_dataset) - 1)\n",
    "# transformed_image, _ = custom_test_dataset[random_idx]\n",
    "# generated_caption = generate_caption(model, transformed_image, tokenizer, device)\n",
    "#\n",
    "# original_image = inverse_transform(transformed_image).permute(1, 2, 0).numpy()\n",
    "# original_image = np.clip(original_image, 0, 1)\n",
    "#\n",
    "# print('Generated caption:', generated_caption)\n",
    "# display_image(original_image, generated_caption)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
