{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:13:52.842379Z",
     "end_time": "2023-04-26T20:13:54.607131Z"
    }
   },
   "outputs": [],
   "source": [
    "# just checking\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406],\n",
    "              std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:13:57.113389Z",
     "end_time": "2023-04-26T20:13:57.146918Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CaptionPreprocessor:\n",
    "    def __init__(self, captions, tokenizer, max_caption_length=20):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_caption_length = max_caption_length\n",
    "        self.captions_tokenized = self.tokenize_captions(captions)\n",
    "\n",
    "    def preprocess(self, caption):\n",
    "        tokens = self.tokenizer.tokenize(caption)\n",
    "        caption_indices = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        if len(caption_indices) < self.max_caption_length:\n",
    "            caption_indices += [self.tokenizer.pad_token_id] * (self.max_caption_length - len(caption_indices))\n",
    "\n",
    "        return caption_indices[:self.max_caption_length]\n",
    "\n",
    "    def tokenize_captions(self, captions):\n",
    "        return [self.preprocess(caption) for caption in captions]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:13:59.326579Z",
     "end_time": "2023-04-26T20:13:59.359737Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomCocoDataset(Dataset):\n",
    "    def __init__(self, coco_dataset, caption_preprocessor, num_captions=5):\n",
    "        self.coco_dataset = coco_dataset\n",
    "        self.caption_preprocessor = caption_preprocessor\n",
    "        self.num_captions = num_captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, caption_list = self.coco_dataset[idx]\n",
    "        selected_caption = random.choice(caption_list[:self.num_captions])\n",
    "        preprocessed_caption = torch.tensor(self.caption_preprocessor.preprocess(selected_caption))\n",
    "        return img, preprocessed_caption"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:00.181547Z",
     "end_time": "2023-04-26T20:14:00.215664Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        init.xavier_uniform_(self.proj.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, num_layers, num_heads, mlp_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(patch_size, in_channels, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, (224 // patch_size) * (224 // patch_size) + 1, embed_dim))\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embed_dim, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.classification_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.positional_encoding[:, :-1]\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:00.672123Z",
     "end_time": "2023-04-26T20:14:00.707489Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.encoding = nn.Parameter(torch.zeros(1, max_len, d_model), requires_grad=False)\n",
    "\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, :, 0::2] = torch.sin(pos * div_term)\n",
    "        self.encoding[:, :, 1::2] = torch.cos(pos * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.encoding[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerCaptionDecoder(nn.Module):\n",
    "    def __init__(self, auto_model, d_model, num_layers, num_heads, mlp_dim, max_len=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.auto_model = auto_model\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(d_model, self.auto_model.config.vocab_size)\n",
    "        init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, captions, memory):\n",
    "        captions = self.auto_model.embeddings(captions)\n",
    "        captions = self.positional_encoding(captions)\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            captions = layer(captions, memory)\n",
    "\n",
    "        logits = self.output_layer(captions)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:02.351329Z",
     "end_time": "2023-04-26T20:14:02.410369Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, image_encoder, caption_decoder):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.caption_decoder = caption_decoder\n",
    "        self.start_token_index = caption_decoder.auto_model.config.bos_token_id or 0\n",
    "        self.embedding_size = caption_decoder.auto_model.config.hidden_size\n",
    "        self.image_feature_linear = nn.Linear(768, self.embedding_size)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        image_features = self.image_encoder(images)\n",
    "        num_patches = (224 // 16) * (224 // 16)\n",
    "        # image_features_flattened = image_features.permute(1, 0, 2).reshape(-1, num_patches, self.embedding_size)\n",
    "\n",
    "        start_token_tensor = torch.tensor([self.start_token_index], dtype=torch.long, device=images.device)\n",
    "        start_token_embeddings = self.caption_decoder.auto_model.embeddings(start_token_tensor).repeat(image_features.shape[0], 1, 1) # getting start token embedding and repeating it for batch size\n",
    "        image_features_summed = image_features.sum(dim=1).unsqueeze(1)\n",
    "        image_features_summed = self.image_feature_linear(image_features_summed)\n",
    "        memory = torch.cat([start_token_embeddings, image_features_summed], dim=1) # Concatenate the start token embeddings with the flattened image features\n",
    "\n",
    "        memory = memory.transpose(0, 1)\n",
    "        captions = captions.transpose(0, 1)\n",
    "\n",
    "        output = self.caption_decoder(captions, memory)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:03.513578Z",
     "end_time": "2023-04-26T20:14:03.594304Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self.learning_rate()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            if param_group['lr'] != lr:\n",
    "                print(f\"Learning rate changed: {param_group['lr']} -> {lr}\")\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def learning_rate(self):\n",
    "        arg1 = self.current_step ** -0.5\n",
    "        arg2 = min(self.current_step * self.warmup_steps ** -1.5, 1)\n",
    "        return (self.d_model ** -0.5) * min(arg1, arg2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:04.110615Z",
     "end_time": "2023-04-26T20:14:04.138068Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_and_save(train_losses, val_losses, learning_rates, max_min_loss_diffs):\n",
    "    plt.style.use('classic')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(train_losses, label='Train Loss')\n",
    "    ax.plot(val_losses, label='Validation Loss')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Loss', fontsize=14)\n",
    "    ax.set_title('Training and Validation Losses', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('losses.png')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(learning_rates, label='Learning Rate')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Learning Rate', fontsize=14)\n",
    "    ax.set_title('Learning Rate Schedule', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('learning_rates.png')\n",
    "\n",
    "    fig_ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(max_min_loss_diffs, label='Loss Difference')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Loss Difference', fontsize=14)\n",
    "    ax.set_title('Difference Between Max and Min Loss per Epoch', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('loss_differences.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:05.305168Z",
     "end_time": "2023-04-26T20:14:05.338900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "train_dataset = CocoCaptions(root='./coco/images',\n",
    "                       annFile='./coco/annotations/captions_train2014.json',\n",
    "                       transform=image_transform)\n",
    "val_dataset = CocoCaptions(root='./coco/images',\n",
    "                           annFile='./coco/annotations/captions_val2014.json',\n",
    "                           transform=image_transform)\n",
    "train_captions = [entry['caption'] for entry in train_dataset.coco.anns.values()]\n",
    "val_captions = [entry['caption'] for entry in val_dataset.coco.anns.values()]\n",
    "\n",
    "caption_preprocessor = CaptionPreprocessor(train_captions + val_captions, tokenizer)\n",
    "\n",
    "max_caption_length_train = max([len(tokenized_caption) for tokenized_caption in caption_preprocessor.tokenize_captions(train_captions)])\n",
    "max_caption_length_val = max([len(tokenized_caption) for tokenized_caption in caption_preprocessor.tokenize_captions(val_captions)])\n",
    "max_caption_length = max(max_caption_length_train, max_caption_length_val)\n",
    "print('Maximum caption length (without <start>, <end>, and <pad> tokens):', max_caption_length)\n",
    "\n",
    "custom_train_dataset = CustomCocoDataset(train_dataset, caption_preprocessor, num_captions=5)\n",
    "custom_val_dataset = CustomCocoDataset(val_dataset, caption_preprocessor, num_captions=5)\n",
    "\n",
    "batch_size = 64\n",
    "train_data_loader = DataLoader(custom_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "val_data_loader = DataLoader(custom_val_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:07.447574Z",
     "end_time": "2023-04-26T20:15:20.822440Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "image_encoder = VisionTransformer(in_channels=3,\n",
    "                                  patch_size=16,\n",
    "                                  embed_dim=768,\n",
    "                                  num_layers=16,\n",
    "                                  num_heads=16,\n",
    "                                  mlp_dim=1024,\n",
    "                                  num_classes=768).to(device)\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(tokenizer_name).to(device)\n",
    "caption_decoder = TransformerCaptionDecoder(auto_model=auto_model,\n",
    "                                            d_model=768,\n",
    "                                            num_layers=16,\n",
    "                                            num_heads=16,\n",
    "                                            mlp_dim=1024).to(device)\n",
    "\n",
    "model = ImageCaptioningModel(image_encoder, caption_decoder).to(device)\n",
    "\n",
    "useTwoGPUs = True\n",
    "if torch.cuda.device_count() > 1 and useTwoGPUs:\n",
    "    print(f'Using {torch.cuda.device_count()} GPUs')\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "total_samples = len(train_data_loader.dataset)\n",
    "batch_size = train_data_loader.batch_size\n",
    "max_iterations = math.ceil(total_samples / batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=2, verbose=True)\n",
    "# scheduler = NoamScheduler(optimizer, d_model=1600, warmup_steps=4000)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=int(num_epochs / 5), eta_min=1e-6)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "learning_rates = []\n",
    "max_min_loss_diffs = []\n",
    "save_name = ''\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch, avg_every):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    last_x_losses = []\n",
    "    for i, (images, captions) in enumerate(tqdm(dataloader, desc='Training')):\n",
    "    # for i, (images, captions) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        captions_input = captions[:, :-1].to(device)\n",
    "        captions_target = captions[:, 1:].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images, captions_input)\n",
    "\n",
    "        loss = criterion(output.reshape(-1, 30522), captions_target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        last_x_losses.append(loss.item())\n",
    "\n",
    "        if i % avg_every == 0:\n",
    "            avg_loss = sum(last_x_losses) / len(last_x_losses)\n",
    "            print(f'Epoch: {epoch+1}, Iteration: {i}, Loss (last {avg_every} iterations: {avg_loss}')\n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in tqdm(dataloader, desc='Validating'):\n",
    "        # for images, captions in dataloader:\n",
    "            image = images.to(device)\n",
    "            captions_input = captions[:, :-1].to(device)\n",
    "            captions_target = captions[:, 1:].to(device)\n",
    "\n",
    "            output = model(images, captions_input)\n",
    "            loss = criterion(output.reshape(-1, 30522), captions_target.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "\n",
    "print('**********STARTING TRAINING**********')\n",
    "training_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    epoch_max_loss = float('-inf')\n",
    "    epoch_min_loss = float('inf')\n",
    "\n",
    "    print(f'Total samples: {total_samples}, Batch size: {batch_size}, Maximum iterations: {max_iterations}')\n",
    "\n",
    "    avg_every = 25\n",
    "    train_loss = train_one_epoch(model, train_data_loader, criterion, optimizer, device, epoch, avg_every)\n",
    "    val_loss = evaluate(model, val_data_loader, criterion, device)\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    print(f'Epoch {epoch+1} total time: {epoch_end - epoch_start}')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "        save_name = f'best_loss_model_{epoch}.pt'\n",
    "        torch.save(model.state_dict(), save_name)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "training_end = time.time()\n",
    "print(f'Total training time: {training_end - training_start}')\n",
    "\n",
    "plot_and_save(train_losses, val_losses, learning_rates, max_min_loss_diffs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T19:40:55.732690Z",
     "end_time": "2023-04-26T19:40:55.732961Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "    # epoch_train_start = time.time()\n",
    "    # for i, (images, captions) in enumerate(train_data_loader):\n",
    "    #     images = images.to(device)\n",
    "    #     captions_input = captions[:, :-1].to(device)\n",
    "    #     captions_target = captions[:, 1:].to(device)\n",
    "    #\n",
    "    #     optimizer.zero_grad()\n",
    "    #     output = model(images, captions_input)\n",
    "    #\n",
    "    #     loss = criterion(output.reshape(-1, 28796), captions_target.view(-1))\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #\n",
    "    #     train_loss += loss.item()\n",
    "    #\n",
    "    #     if loss.item() > epoch_max_loss:\n",
    "    #         epoch_max_loss = loss.item()\n",
    "    #         print(f'Max loss set to: {epoch_max_loss}')\n",
    "    #     if loss.item() < epoch_min_loss:\n",
    "    #         epoch_min_loss = loss.item()\n",
    "    #         print(f'Min loss set to: {epoch_min_loss}')\n",
    "    #\n",
    "    #     if i % 50 == 0:\n",
    "    #         print(f'Epoch: {epoch+1}/{num_epochs}, Iteration: {i}, Loss: {loss.item()}')\n",
    "    #\n",
    "    # epoch_train_end = time.time()\n",
    "    # epoch_train_time = epoch_train_end - epoch_train_start\n",
    "    # print(f'Epoch {epoch+1} training time: {epoch_train_time}')\n",
    "    #\n",
    "    # epoch_max_min_diff = epoch_max_loss - epoch_min_loss\n",
    "    # if epoch + 1 != 1:\n",
    "    #     max_min_loss_diffs.append(epoch_max_min_diff)\n",
    "    # print(f'Difference between max and min loss in epoch {epoch+1}: {epoch_max_min_diff}')\n",
    "    #\n",
    "    # train_loss /= len(train_data_loader)\n",
    "    #\n",
    "    # model.eval()\n",
    "    # val_loss = 0\n",
    "\n",
    "#     epoch_val_start = time.time()\n",
    "#     with torch.no_grad():\n",
    "#         for images, captions in val_data_loader:\n",
    "#             images = images.to(device)\n",
    "#             captions_input = captions[:, :-1].to(device)\n",
    "#             captions_target = captions[:, 1:].to(device)\n",
    "#\n",
    "#             output = model(images, captions_input)\n",
    "#             loss = criterion(output.reshape(-1, 28796), captions_target.view(-1))\n",
    "#\n",
    "#             val_loss += loss.item()\n",
    "#\n",
    "#     epoch_val_end = time.time()\n",
    "#     epoch_val_time = epoch_val_end - epoch_val_start\n",
    "#     print(f'Epoch {epoch+1} validation time: {epoch_val_time}')\n",
    "#\n",
    "#     epoch_end = time.time()\n",
    "#     epoch_time = epoch_end - epoch_start\n",
    "#     print(f'Epoch {epoch+1} total time: {epoch_time}')\n",
    "#\n",
    "#     val_loss /= len(val_data_loader)\n",
    "#     print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "#     train_losses.append(train_loss)\n",
    "#     val_losses.append(val_loss)\n",
    "#     learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "#\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#\n",
    "#         if os.path.exists(save_name):\n",
    "#             os.remove(save_name)\n",
    "#\n",
    "#         save_name = f'best_loss_model_{epoch}.pth'\n",
    "#         torch.save(model.state_dict(), save_name)\n",
    "#\n",
    "#     scheduler.step()\n",
    "#\n",
    "# training_end = time.time()\n",
    "# training_time = training_end - training_start\n",
    "# print(f'Total training time: {training_time}')\n",
    "#\n",
    "# plot_and_save(train_losses, val_losses, learning_rates, max_min_loss_diffs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
