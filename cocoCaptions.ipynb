{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:13:52.842379Z",
     "end_time": "2023-04-26T20:13:54.607131Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "image_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406],\n",
    "              std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:13:57.113389Z",
     "end_time": "2023-04-26T20:13:57.146918Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class CaptionPreprocessor:\n",
    "    def __init__(self, captions, tokenizer, max_caption_length=20):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_caption_length = max_caption_length\n",
    "        self.captions_tokenized = self.tokenize_captions(captions)\n",
    "\n",
    "    def preprocess(self, caption):\n",
    "        tokens = self.tokenizer.tokenize(caption)\n",
    "        caption_indices = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        if len(caption_indices) < self.max_caption_length:\n",
    "            caption_indices += [self.tokenizer.pad_token_id] * (self.max_caption_length - len(caption_indices))\n",
    "\n",
    "        return caption_indices[:self.max_caption_length]\n",
    "\n",
    "    def tokenize_captions(self, captions):\n",
    "        return [self.preprocess(caption) for caption in captions]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:13:59.326579Z",
     "end_time": "2023-04-26T20:13:59.359737Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class CustomCocoDataset(Dataset):\n",
    "    def __init__(self, coco_dataset, caption_preprocessor, num_captions=5):\n",
    "        self.coco_dataset = coco_dataset\n",
    "        self.caption_preprocessor = caption_preprocessor\n",
    "        self.num_captions = num_captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, caption_list = self.coco_dataset[idx]\n",
    "        selected_caption = random.choice(caption_list[:self.num_captions])\n",
    "        preprocessed_caption = torch.tensor(self.caption_preprocessor.preprocess(selected_caption))\n",
    "        return img, preprocessed_caption"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:00.181547Z",
     "end_time": "2023-04-26T20:14:00.215664Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        init.xavier_uniform_(self.proj.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, num_layers, num_heads, mlp_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(patch_size, in_channels, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, (224 // patch_size) * (224 // patch_size) + 1, embed_dim))\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embed_dim, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.classification_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.positional_encoding[:, :-1]\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:00.672123Z",
     "end_time": "2023-04-26T20:14:00.707489Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.encoding = nn.Parameter(torch.zeros(1, max_len, d_model), requires_grad=False)\n",
    "\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, :, 0::2] = torch.sin(pos * div_term)\n",
    "        self.encoding[:, :, 1::2] = torch.cos(pos * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.encoding[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerCaptionDecoder(nn.Module):\n",
    "    def __init__(self, auto_model, d_model, num_layers, num_heads, mlp_dim, max_len=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.auto_model = auto_model\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(d_model, self.auto_model.config.vocab_size)\n",
    "        init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, captions, memory):\n",
    "        captions = self.auto_model.embeddings(captions)\n",
    "        captions = self.positional_encoding(captions)\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            captions = layer(captions, memory)\n",
    "\n",
    "        logits = self.output_layer(captions)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:02.351329Z",
     "end_time": "2023-04-26T20:14:02.410369Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, image_encoder, caption_decoder):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.caption_decoder = caption_decoder\n",
    "        self.start_token_index = caption_decoder.auto_model.config.bos_token_id or 0\n",
    "        self.embedding_size = caption_decoder.auto_model.config.hidden_size\n",
    "        self.image_feature_linear = nn.Linear(768, self.embedding_size)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        image_features = self.image_encoder(images)\n",
    "        num_patches = (224 // 16) * (224 // 16)\n",
    "        # image_features_flattened = image_features.permute(1, 0, 2).reshape(-1, num_patches, self.embedding_size)\n",
    "\n",
    "        start_token_tensor = torch.tensor([self.start_token_index], dtype=torch.long, device=images.device)\n",
    "        start_token_embeddings = self.caption_decoder.auto_model.embeddings(start_token_tensor).repeat(image_features.shape[0], 1, 1) # getting start token embedding and repeating it for batch size\n",
    "        image_features_summed = image_features.sum(dim=1).unsqueeze(1)\n",
    "        image_features_summed = self.image_feature_linear(image_features_summed)\n",
    "        memory = torch.cat([start_token_embeddings, image_features_summed], dim=1) # Concatenate the start token embeddings with the flattened image features\n",
    "\n",
    "        memory = memory.transpose(0, 1)\n",
    "        captions = captions.transpose(0, 1)\n",
    "\n",
    "        output = self.caption_decoder(captions, memory)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:03.513578Z",
     "end_time": "2023-04-26T20:14:03.594304Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self.learning_rate()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            if param_group['lr'] != lr:\n",
    "                print(f\"Learning rate changed: {param_group['lr']} -> {lr}\")\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def learning_rate(self):\n",
    "        arg1 = self.current_step ** -0.5\n",
    "        arg2 = min(self.current_step * self.warmup_steps ** -1.5, 1)\n",
    "        return (self.d_model ** -0.5) * min(arg1, arg2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:04.110615Z",
     "end_time": "2023-04-26T20:14:04.138068Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def plot_and_save(train_losses, val_losses, learning_rates, max_min_loss_diffs):\n",
    "    plt.style.use('classic')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(train_losses, label='Train Loss')\n",
    "    ax.plot(val_losses, label='Validation Loss')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Loss', fontsize=14)\n",
    "    ax.set_title('Training and Validation Losses', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('losses.png')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(learning_rates, label='Learning Rate')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Learning Rate', fontsize=14)\n",
    "    ax.set_title('Learning Rate Schedule', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('learning_rates.png')\n",
    "\n",
    "    fig_ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(max_min_loss_diffs, label='Loss Difference')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Loss Difference', fontsize=14)\n",
    "    ax.set_title('Difference Between Max and Min Loss per Epoch', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('loss_differences.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:05.305168Z",
     "end_time": "2023-04-26T20:14:05.338900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.44s)\n",
      "creating index...\n",
      "index created!\n",
      "Maximum caption length (without <start>, <end>, and <pad> tokens): 20\n"
     ]
    }
   ],
   "source": [
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "train_dataset = CocoCaptions(root='./coco/images',\n",
    "                       annFile='./coco/annotations/captions_train2014.json',\n",
    "                       transform=image_transform)\n",
    "val_dataset = CocoCaptions(root='./coco/images',\n",
    "                           annFile='./coco/annotations/captions_val2014.json',\n",
    "                           transform=image_transform)\n",
    "train_captions = [entry['caption'] for entry in train_dataset.coco.anns.values()]\n",
    "val_captions = [entry['caption'] for entry in val_dataset.coco.anns.values()]\n",
    "\n",
    "caption_preprocessor = CaptionPreprocessor(train_captions + val_captions, tokenizer)\n",
    "\n",
    "max_caption_length_train = max([len(tokenized_caption) for tokenized_caption in caption_preprocessor.tokenize_captions(train_captions)])\n",
    "max_caption_length_val = max([len(tokenized_caption) for tokenized_caption in caption_preprocessor.tokenize_captions(val_captions)])\n",
    "max_caption_length = max(max_caption_length_train, max_caption_length_val)\n",
    "print('Maximum caption length (without <start>, <end>, and <pad> tokens):', max_caption_length)\n",
    "\n",
    "custom_train_dataset = CustomCocoDataset(train_dataset, caption_preprocessor, num_captions=5)\n",
    "custom_val_dataset = CustomCocoDataset(val_dataset, caption_preprocessor, num_captions=5)\n",
    "\n",
    "batch_size = 64\n",
    "train_data_loader = DataLoader(custom_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "val_data_loader = DataLoader(custom_val_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:07.447574Z",
     "end_time": "2023-04-26T20:15:20.822440Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/aisec-101/anaconda3/envs/coco_captions/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs\n",
      "**********STARTING TRAINING**********\n",
      "Total samples: 82783, Batch size: 64, Maximum iterations: 1294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/1293 [00:08<3:06:05,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 0, Loss (last 25 iterations: 10.325166702270508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 26/1293 [00:29<16:31,  1.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 25, Loss (last 25 iterations: 6.803201216917771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 51/1293 [00:48<16:30,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 50, Loss (last 25 iterations: 6.416073976778517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 76/1293 [01:08<16:30,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 75, Loss (last 25 iterations: 6.273479599701731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 101/1293 [01:28<15:47,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 100, Loss (last 25 iterations: 6.190937552121606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 126/1293 [01:48<15:28,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 125, Loss (last 25 iterations: 6.145129888776749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 151/1293 [02:08<15:09,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 150, Loss (last 25 iterations: 6.111671078284055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▎        | 176/1293 [02:28<14:48,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 175, Loss (last 25 iterations: 6.079453842206434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 201/1293 [02:48<14:55,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 200, Loss (last 25 iterations: 6.0606396233857565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 226/1293 [03:08<14:10,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 225, Loss (last 25 iterations: 6.046436328803543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 251/1293 [03:28<13:50,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 250, Loss (last 25 iterations: 6.029408445396271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██▏       | 276/1293 [03:49<13:31,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 275, Loss (last 25 iterations: 6.015607823496279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 301/1293 [04:09<13:12,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 300, Loss (last 25 iterations: 6.006954096480461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 326/1293 [04:29<13:04,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 325, Loss (last 25 iterations: 5.996802621092533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 351/1293 [04:49<12:33,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 350, Loss (last 25 iterations: 5.989074253288769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 376/1293 [05:09<12:11,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 375, Loss (last 25 iterations: 5.983686061615639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 401/1293 [05:29<11:58,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 400, Loss (last 25 iterations: 5.97716648620263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 426/1293 [05:49<11:32,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 425, Loss (last 25 iterations: 5.97098171990802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 451/1293 [06:10<11:23,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 450, Loss (last 25 iterations: 5.964980954341508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 476/1293 [06:29<10:52,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 475, Loss (last 25 iterations: 5.962292891590535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 501/1293 [06:49<10:31,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 500, Loss (last 25 iterations: 5.956690281926991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 526/1293 [07:10<10:11,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 525, Loss (last 25 iterations: 5.951172574844651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 551/1293 [07:30<09:54,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 550, Loss (last 25 iterations: 5.946928570361406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 576/1293 [07:50<09:38,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 575, Loss (last 25 iterations: 5.941401449342568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▋     | 601/1293 [08:10<09:12,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 600, Loss (last 25 iterations: 5.9383809364179205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 626/1293 [08:30<08:53,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 625, Loss (last 25 iterations: 5.934746167149407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 651/1293 [08:50<08:36,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 650, Loss (last 25 iterations: 5.931481723961193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 676/1293 [09:10<08:12,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 675, Loss (last 25 iterations: 5.9284918117805345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 701/1293 [09:31<07:57,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 700, Loss (last 25 iterations: 5.9253848507128835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 726/1293 [09:51<07:33,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 725, Loss (last 25 iterations: 5.923208065926207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 751/1293 [10:11<07:13,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 750, Loss (last 25 iterations: 5.919688994335271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 776/1293 [10:31<06:53,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 775, Loss (last 25 iterations: 5.915811761752846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 801/1293 [10:51<06:33,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 800, Loss (last 25 iterations: 5.913299864151058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 826/1293 [11:11<06:17,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 825, Loss (last 25 iterations: 5.910734591126153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 851/1293 [11:31<05:52,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 850, Loss (last 25 iterations: 5.908861529532947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 876/1293 [11:51<05:33,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 875, Loss (last 25 iterations: 5.907161130752738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 901/1293 [12:12<05:14,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 900, Loss (last 25 iterations: 5.905238712535185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 926/1293 [12:32<04:52,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 925, Loss (last 25 iterations: 5.903741049200104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▎  | 951/1293 [12:52<04:35,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 950, Loss (last 25 iterations: 5.902740184190271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 976/1293 [13:12<04:12,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 975, Loss (last 25 iterations: 5.901541073302754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 1001/1293 [13:32<03:52,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1000, Loss (last 25 iterations: 5.900335427645322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▉  | 1026/1293 [13:52<03:33,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1025, Loss (last 25 iterations: 5.898841606478477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████▏ | 1051/1293 [14:12<03:13,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1050, Loss (last 25 iterations: 5.897455783938363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 1076/1293 [14:32<02:54,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1075, Loss (last 25 iterations: 5.8961613470736935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 1101/1293 [14:53<02:33,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1100, Loss (last 25 iterations: 5.895112850577262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 1126/1293 [15:13<02:13,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1125, Loss (last 25 iterations: 5.8935130735061945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 1151/1293 [15:33<01:53,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1150, Loss (last 25 iterations: 5.892392571546221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 1176/1293 [15:53<01:33,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1175, Loss (last 25 iterations: 5.892017703478028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 1201/1293 [16:13<01:13,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1200, Loss (last 25 iterations: 5.890774253206785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 1226/1293 [16:33<00:53,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1225, Loss (last 25 iterations: 5.8901653394808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 1251/1293 [16:53<00:33,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1250, Loss (last 25 iterations: 5.889544274309556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▊| 1276/1293 [17:13<00:13,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 1275, Loss (last 25 iterations: 5.888377959078008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1293/1293 [17:27<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1293/1293 [17:27<00:00,  1.23it/s]\n",
      "Validating:   0%|          | 0/632 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 632/632 [03:14<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 total time: 1242.019146680832\n",
      "Total samples: 82783, Batch size: 64, Maximum iterations: 1294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/1293 [00:02<50:19,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 0, Loss (last 25 iterations: 5.904211044311523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 26/1293 [00:22<16:52,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 25, Loss (last 25 iterations: 5.846445028598492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 51/1293 [00:42<16:30,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 50, Loss (last 25 iterations: 5.841522001752667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 76/1293 [01:02<16:10,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 75, Loss (last 25 iterations: 5.843715812030592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 101/1293 [01:22<15:52,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 100, Loss (last 25 iterations: 5.834309913144253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 126/1293 [01:43<15:43,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 125, Loss (last 25 iterations: 5.834685473215012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 151/1293 [02:03<15:15,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 150, Loss (last 25 iterations: 5.834682133024102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▎        | 176/1293 [02:23<14:54,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 175, Loss (last 25 iterations: 5.837395044890317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 201/1293 [02:43<14:39,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 200, Loss (last 25 iterations: 5.839643736976889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 226/1293 [03:03<14:14,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 225, Loss (last 25 iterations: 5.83822950219686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 251/1293 [03:23<14:01,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 250, Loss (last 25 iterations: 5.835040907460855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██▏       | 276/1293 [03:43<13:33,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 275, Loss (last 25 iterations: 5.833148337792659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 301/1293 [04:03<13:10,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 300, Loss (last 25 iterations: 5.833812957586244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 326/1293 [04:24<12:50,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 325, Loss (last 25 iterations: 5.835760985415406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 351/1293 [04:44<12:34,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 350, Loss (last 25 iterations: 5.838027066994257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 376/1293 [05:04<12:17,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 375, Loss (last 25 iterations: 5.838830443138772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 401/1293 [05:24<11:53,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 400, Loss (last 25 iterations: 5.83895943230227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 426/1293 [05:44<11:33,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 425, Loss (last 25 iterations: 5.840618992075674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 451/1293 [06:04<11:12,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 450, Loss (last 25 iterations: 5.837665506054186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 476/1293 [06:24<10:52,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iteration: 475, Loss (last 25 iterations: 5.836724000818589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 483/1293 [06:31<10:56,  1.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 101\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTotal samples: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_samples\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Batch size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Maximum iterations: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_iterations\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    100\u001B[0m avg_every \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m25\u001B[39m\n\u001B[0;32m--> 101\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mavg_every\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    102\u001B[0m val_loss \u001B[38;5;241m=\u001B[39m evaluate(model, val_data_loader, criterion, device)\n\u001B[1;32m    104\u001B[0m epoch_end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "Cell \u001B[0;32mIn[11], line 63\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(model, dataloader, criterion, optimizer, device, epoch, avg_every)\u001B[0m\n\u001B[1;32m     61\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m30522\u001B[39m), captions_target\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     62\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 63\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     66\u001B[0m last_x_losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/anaconda3/envs/coco_captions/lib/python3.10/site-packages/torch/optim/optimizer.py:109\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    107\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m--> 109\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/coco_captions/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/coco_captions/lib/python3.10/site-packages/torch/optim/adam.py:157\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    153\u001B[0m                 max_exp_avg_sqs\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_exp_avg_sq\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m    155\u001B[0m             state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m--> 157\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m         \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m         \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m         \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m         \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m         \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m         \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m         \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m         \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m         \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m         \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m         \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m         \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m         \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m         \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/anaconda3/envs/coco_captions/lib/python3.10/site-packages/torch/optim/adam.py:213\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 213\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    216\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/coco_captions/lib/python3.10/site-packages/torch/optim/adam.py:265\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001B[0m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[1;32m    264\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mmul_(beta1)\u001B[38;5;241m.\u001B[39madd_(grad, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[0;32m--> 265\u001B[0m \u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad\u001B[38;5;241m.\u001B[39mconj(), value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable:\n\u001B[1;32m    268\u001B[0m     step \u001B[38;5;241m=\u001B[39m step_t\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "image_encoder = VisionTransformer(in_channels=3,\n",
    "                                  patch_size=16,\n",
    "                                  embed_dim=768,\n",
    "                                  num_layers=16,\n",
    "                                  num_heads=16,\n",
    "                                  mlp_dim=1024,\n",
    "                                  num_classes=768).to(device)\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(tokenizer_name).to(device)\n",
    "caption_decoder = TransformerCaptionDecoder(auto_model=auto_model,\n",
    "                                            d_model=768,\n",
    "                                            num_layers=16,\n",
    "                                            num_heads=16,\n",
    "                                            mlp_dim=1024).to(device)\n",
    "\n",
    "model = ImageCaptioningModel(image_encoder, caption_decoder).to(device)\n",
    "\n",
    "useTwoGPUs = True\n",
    "if torch.cuda.device_count() > 1 and useTwoGPUs:\n",
    "    print(f'Using {torch.cuda.device_count()} GPUs')\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "total_samples = len(train_data_loader.dataset)\n",
    "batch_size = train_data_loader.batch_size\n",
    "max_iterations = math.ceil(total_samples / batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=2, verbose=True)\n",
    "# scheduler = NoamScheduler(optimizer, d_model=1600, warmup_steps=4000)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=int(num_epochs / 5), eta_min=1e-6)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "learning_rates = []\n",
    "max_min_loss_diffs = []\n",
    "save_name = ''\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch, avg_every):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    last_x_losses = []\n",
    "    for i, (images, captions) in enumerate(tqdm(dataloader, desc='Training')):\n",
    "    # for i, (images, captions) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        captions_input = captions[:, :-1].to(device)\n",
    "        captions_target = captions[:, 1:].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images, captions_input)\n",
    "\n",
    "        loss = criterion(output.reshape(-1, 30522), captions_target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        last_x_losses.append(loss.item())\n",
    "\n",
    "        if i % avg_every == 0:\n",
    "            avg_loss = sum(last_x_losses) / len(last_x_losses)\n",
    "            print(f'Epoch: {epoch+1}, Iteration: {i}, Loss (last {avg_every} iterations: {avg_loss}')\n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in tqdm(dataloader, desc='Validating'):\n",
    "        # for images, captions in dataloader:\n",
    "            image = images.to(device)\n",
    "            captions_input = captions[:, :-1].to(device)\n",
    "            captions_target = captions[:, 1:].to(device)\n",
    "\n",
    "            output = model(images, captions_input)\n",
    "            loss = criterion(output.reshape(-1, 30522), captions_target.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "\n",
    "print('**********STARTING TRAINING**********')\n",
    "training_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    epoch_max_loss = float('-inf')\n",
    "    epoch_min_loss = float('inf')\n",
    "\n",
    "    print(f'Total samples: {total_samples}, Batch size: {batch_size}, Maximum iterations: {max_iterations}')\n",
    "\n",
    "    avg_every = 25\n",
    "    train_loss = train_one_epoch(model, train_data_loader, criterion, optimizer, device, epoch, avg_every)\n",
    "    val_loss = evaluate(model, val_data_loader, criterion, device)\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    print(f'Epoch {epoch+1} total time: {epoch_end - epoch_start}')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "        save_name = f'best_loss_model_{epoch}.pt'\n",
    "        torch.save(model.state_dict(), save_name)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "training_end = time.time()\n",
    "print(f'Total training time: {training_end - training_start}')\n",
    "\n",
    "plot_and_save(train_losses, val_losses, learning_rates, max_min_loss_diffs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T19:40:55.732690Z",
     "end_time": "2023-04-26T19:40:55.732961Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "    # epoch_train_start = time.time()\n",
    "    # for i, (images, captions) in enumerate(train_data_loader):\n",
    "    #     images = images.to(device)\n",
    "    #     captions_input = captions[:, :-1].to(device)\n",
    "    #     captions_target = captions[:, 1:].to(device)\n",
    "    #\n",
    "    #     optimizer.zero_grad()\n",
    "    #     output = model(images, captions_input)\n",
    "    #\n",
    "    #     loss = criterion(output.reshape(-1, 28796), captions_target.view(-1))\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #\n",
    "    #     train_loss += loss.item()\n",
    "    #\n",
    "    #     if loss.item() > epoch_max_loss:\n",
    "    #         epoch_max_loss = loss.item()\n",
    "    #         print(f'Max loss set to: {epoch_max_loss}')\n",
    "    #     if loss.item() < epoch_min_loss:\n",
    "    #         epoch_min_loss = loss.item()\n",
    "    #         print(f'Min loss set to: {epoch_min_loss}')\n",
    "    #\n",
    "    #     if i % 50 == 0:\n",
    "    #         print(f'Epoch: {epoch+1}/{num_epochs}, Iteration: {i}, Loss: {loss.item()}')\n",
    "    #\n",
    "    # epoch_train_end = time.time()\n",
    "    # epoch_train_time = epoch_train_end - epoch_train_start\n",
    "    # print(f'Epoch {epoch+1} training time: {epoch_train_time}')\n",
    "    #\n",
    "    # epoch_max_min_diff = epoch_max_loss - epoch_min_loss\n",
    "    # if epoch + 1 != 1:\n",
    "    #     max_min_loss_diffs.append(epoch_max_min_diff)\n",
    "    # print(f'Difference between max and min loss in epoch {epoch+1}: {epoch_max_min_diff}')\n",
    "    #\n",
    "    # train_loss /= len(train_data_loader)\n",
    "    #\n",
    "    # model.eval()\n",
    "    # val_loss = 0\n",
    "\n",
    "#     epoch_val_start = time.time()\n",
    "#     with torch.no_grad():\n",
    "#         for images, captions in val_data_loader:\n",
    "#             images = images.to(device)\n",
    "#             captions_input = captions[:, :-1].to(device)\n",
    "#             captions_target = captions[:, 1:].to(device)\n",
    "#\n",
    "#             output = model(images, captions_input)\n",
    "#             loss = criterion(output.reshape(-1, 28796), captions_target.view(-1))\n",
    "#\n",
    "#             val_loss += loss.item()\n",
    "#\n",
    "#     epoch_val_end = time.time()\n",
    "#     epoch_val_time = epoch_val_end - epoch_val_start\n",
    "#     print(f'Epoch {epoch+1} validation time: {epoch_val_time}')\n",
    "#\n",
    "#     epoch_end = time.time()\n",
    "#     epoch_time = epoch_end - epoch_start\n",
    "#     print(f'Epoch {epoch+1} total time: {epoch_time}')\n",
    "#\n",
    "#     val_loss /= len(val_data_loader)\n",
    "#     print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "#     train_losses.append(train_loss)\n",
    "#     val_losses.append(val_loss)\n",
    "#     learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "#\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#\n",
    "#         if os.path.exists(save_name):\n",
    "#             os.remove(save_name)\n",
    "#\n",
    "#         save_name = f'best_loss_model_{epoch}.pth'\n",
    "#         torch.save(model.state_dict(), save_name)\n",
    "#\n",
    "#     scheduler.step()\n",
    "#\n",
    "# training_end = time.time()\n",
    "# training_time = training_end - training_start\n",
    "# print(f'Total training time: {training_time}')\n",
    "#\n",
    "# plot_and_save(train_losses, val_losses, learning_rates, max_min_loss_diffs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
