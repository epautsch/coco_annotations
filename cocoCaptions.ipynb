{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-26T23:08:40.154528Z",
     "end_time": "2023-04-26T23:08:46.714317Z"
    }
   },
   "outputs": [],
   "source": [
    "# just checking\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "lr = 1e-2\n",
    "epochs = 300\n",
    "eps = []\n",
    "lrs = []\n",
    "for i in range(epochs):\n",
    "    eps.append(i)\n",
    "    lrs.append(lr)\n",
    "    lr *= 0.9\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(eps, lrs)\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T23:20:09.263936Z",
     "end_time": "2023-04-26T23:20:09.579965Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406],\n",
    "              std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:13:57.113389Z",
     "end_time": "2023-04-26T20:13:57.146918Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CaptionPreprocessor:\n",
    "    def __init__(self, captions, tokenizer, max_caption_length=20):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_caption_length = max_caption_length\n",
    "        self.captions_tokenized = self.tokenize_captions(captions)\n",
    "\n",
    "    def preprocess(self, caption):\n",
    "        tokens = self.tokenizer.tokenize(caption)\n",
    "        caption_indices = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        if len(caption_indices) < self.max_caption_length:\n",
    "            caption_indices += [self.tokenizer.pad_token_id] * (self.max_caption_length - len(caption_indices))\n",
    "\n",
    "        return caption_indices[:self.max_caption_length]\n",
    "\n",
    "    def tokenize_captions(self, captions):\n",
    "        return [self.preprocess(caption) for caption in captions]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:13:59.326579Z",
     "end_time": "2023-04-26T20:13:59.359737Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomCocoDataset(Dataset):\n",
    "    def __init__(self, coco_dataset, caption_preprocessor, num_captions=5):\n",
    "        self.coco_dataset = coco_dataset\n",
    "        self.caption_preprocessor = caption_preprocessor\n",
    "        self.num_captions = num_captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, caption_list = self.coco_dataset[idx]\n",
    "        selected_caption = random.choice(caption_list[:self.num_captions])\n",
    "        preprocessed_caption = torch.tensor(self.caption_preprocessor.preprocess(selected_caption))\n",
    "        return img, preprocessed_caption"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:00.181547Z",
     "end_time": "2023-04-26T20:14:00.215664Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        init.xavier_uniform_(self.proj.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, num_layers, num_heads, mlp_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(patch_size, in_channels, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, (224 // patch_size) * (224 // patch_size) + 1, embed_dim))\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embed_dim, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.classification_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.positional_encoding[:, :-1]\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:00.672123Z",
     "end_time": "2023-04-26T20:14:00.707489Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.encoding = nn.Parameter(torch.zeros(1, max_len, d_model), requires_grad=False)\n",
    "\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, :, 0::2] = torch.sin(pos * div_term)\n",
    "        self.encoding[:, :, 1::2] = torch.cos(pos * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.encoding[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerCaptionDecoder(nn.Module):\n",
    "    def __init__(self, auto_model, d_model, num_layers, num_heads, mlp_dim, max_len=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.auto_model = auto_model\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(d_model, self.auto_model.config.vocab_size)\n",
    "        init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, captions, memory):\n",
    "        captions = self.auto_model.embeddings(captions)\n",
    "        captions = self.positional_encoding(captions)\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            captions = layer(captions, memory)\n",
    "\n",
    "        logits = self.output_layer(captions)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:02.351329Z",
     "end_time": "2023-04-26T20:14:02.410369Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, image_encoder, caption_decoder):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.caption_decoder = caption_decoder\n",
    "        self.start_token_index = caption_decoder.auto_model.config.bos_token_id or 0\n",
    "        self.embedding_size = caption_decoder.auto_model.config.hidden_size\n",
    "        self.image_feature_linear = nn.Linear(768, self.embedding_size)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        image_features = self.image_encoder(images)\n",
    "        num_patches = (224 // 16) * (224 // 16)\n",
    "        # image_features_flattened = image_features.permute(1, 0, 2).reshape(-1, num_patches, self.embedding_size)\n",
    "\n",
    "        start_token_tensor = torch.tensor([self.start_token_index], dtype=torch.long, device=images.device)\n",
    "        start_token_embeddings = self.caption_decoder.auto_model.embeddings(start_token_tensor).repeat(image_features.shape[0], 1, 1) # getting start token embedding and repeating it for batch size\n",
    "        image_features_summed = image_features.sum(dim=1).unsqueeze(1)\n",
    "        image_features_summed = self.image_feature_linear(image_features_summed)\n",
    "        memory = torch.cat([start_token_embeddings, image_features_summed], dim=1) # Concatenate the start token embeddings with the flattened image features\n",
    "\n",
    "        memory = memory.transpose(0, 1)\n",
    "        captions = captions.transpose(0, 1)\n",
    "\n",
    "        output = self.caption_decoder(captions, memory)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:03.513578Z",
     "end_time": "2023-04-26T20:14:03.594304Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch, avg_every):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    last_x_losses = []\n",
    "    for i, (images, captions) in enumerate(tqdm(dataloader, desc='Training')):\n",
    "    # for i, (images, captions) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        captions_input = captions[:, :-1].to(device)\n",
    "        captions_target = captions[:, 1:].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images, captions_input)\n",
    "\n",
    "        loss = criterion(output.reshape(-1, 30522), captions_target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        last_x_losses.append(loss.item())\n",
    "\n",
    "        if i % avg_every == 0:\n",
    "            avg_loss = sum(last_x_losses) / len(last_x_losses)\n",
    "            print(f'Epoch: {epoch+1}, Iteration: {i}, Loss (last {avg_every} iterations): {avg_loss:.4f}')\n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in tqdm(dataloader, desc='Validating'):\n",
    "        # for images, captions in dataloader:\n",
    "            image = images.to(device)\n",
    "            captions_input = captions[:, :-1].to(device)\n",
    "            captions_target = captions[:, 1:].to(device)\n",
    "\n",
    "            output = model(images, captions_input)\n",
    "            loss = criterion(output.reshape(-1, 30522), captions_target.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self.learning_rate()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            if param_group['lr'] != lr:\n",
    "                print(f\"Learning rate changed: {param_group['lr']} -> {lr}\")\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def learning_rate(self):\n",
    "        arg1 = self.current_step ** -0.5\n",
    "        arg2 = min(self.current_step * self.warmup_steps ** -1.5, 1)\n",
    "        return (self.d_model ** -0.5) * min(arg1, arg2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:04.110615Z",
     "end_time": "2023-04-26T20:14:04.138068Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_and_save(train_losses, val_losses, learning_rates, max_min_loss_diffs):\n",
    "    plt.style.use('classic')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(train_losses, label='Train Loss')\n",
    "    ax.plot(val_losses, label='Validation Loss')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Loss', fontsize=14)\n",
    "    ax.set_title('Training and Validation Losses', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('losses.png')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(learning_rates, label='Learning Rate')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Learning Rate', fontsize=14)\n",
    "    ax.set_title('Learning Rate Schedule', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('learning_rates.png')\n",
    "\n",
    "    fig_ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(max_min_loss_diffs, label='Loss Difference')\n",
    "    ax.set_xlabel('Epochs', fontsize=14)\n",
    "    ax.set_ylabel('Loss Difference', fontsize=14)\n",
    "    ax.set_title('Difference Between Max and Min Loss per Epoch', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig('loss_differences.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:05.305168Z",
     "end_time": "2023-04-26T20:14:05.338900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "train_dataset = CocoCaptions(root='./coco/images',\n",
    "                       annFile='./coco/annotations/captions_train2014.json',\n",
    "                       transform=image_transform)\n",
    "val_dataset = CocoCaptions(root='./coco/images',\n",
    "                           annFile='./coco/annotations/captions_val2014.json',\n",
    "                           transform=image_transform)\n",
    "train_captions = [entry['caption'] for entry in train_dataset.coco.anns.values()]\n",
    "val_captions = [entry['caption'] for entry in val_dataset.coco.anns.values()]\n",
    "\n",
    "caption_preprocessor = CaptionPreprocessor(train_captions + val_captions, tokenizer)\n",
    "\n",
    "max_caption_length_train = max([len(tokenized_caption) for tokenized_caption in caption_preprocessor.tokenize_captions(train_captions)])\n",
    "max_caption_length_val = max([len(tokenized_caption) for tokenized_caption in caption_preprocessor.tokenize_captions(val_captions)])\n",
    "max_caption_length = max(max_caption_length_train, max_caption_length_val)\n",
    "print('Maximum caption length (without <start>, <end>, and <pad> tokens):', max_caption_length)\n",
    "\n",
    "custom_train_dataset = CustomCocoDataset(train_dataset, caption_preprocessor, num_captions=5)\n",
    "custom_val_dataset = CustomCocoDataset(val_dataset, caption_preprocessor, num_captions=5)\n",
    "\n",
    "batch_size = 64\n",
    "train_data_loader = DataLoader(custom_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "val_data_loader = DataLoader(custom_val_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T20:14:07.447574Z",
     "end_time": "2023-04-26T20:15:20.822440Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "image_encoder = VisionTransformer(in_channels=3,\n",
    "                                  patch_size=16,\n",
    "                                  embed_dim=768,\n",
    "                                  num_layers=4,\n",
    "                                  num_heads=16,\n",
    "                                  mlp_dim=512,\n",
    "                                  num_classes=768).to(device)\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(tokenizer_name).to(device)\n",
    "caption_decoder = TransformerCaptionDecoder(auto_model=auto_model,\n",
    "                                            d_model=768,\n",
    "                                            num_layers=4,\n",
    "                                            num_heads=16,\n",
    "                                            mlp_dim=512).to(device)\n",
    "\n",
    "model = ImageCaptioningModel(image_encoder, caption_decoder).to(device)\n",
    "\n",
    "useTwoGPUs = True\n",
    "if torch.cuda.device_count() > 1 and useTwoGPUs:\n",
    "    print(f'Using {torch.cuda.device_count()} GPUs')\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "total_samples = len(train_data_loader.dataset)\n",
    "batch_size = train_data_loader.batch_size\n",
    "max_iterations = math.ceil(total_samples / batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  #, weight_decay=1e-5)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.67, patience=2, verbose=True)\n",
    "# scheduler = NoamScheduler(optimizer, d_model=1600, warmup_steps=4000)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=int(num_epochs / 5), eta_min=1e-6)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "learning_rates = []\n",
    "max_min_loss_diffs = []\n",
    "\n",
    "load_best_model = True\n",
    "best_model_path = 'best_loss_model1.pt'\n",
    "if load_best_model and os.path.exists(best_model_path):\n",
    "    state_dict = torch.load(best_model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    print('Loaded best saved model...')\n",
    "    best_val_loss = evaluate(model, val_data_loader, criterion, device)\n",
    "    print(f'Validation loss of hte loaded model: {best_val_loss:.4f}')\n",
    "\n",
    "print('**********STARTING TRAINING**********')\n",
    "training_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    epoch_max_loss = float('-inf')\n",
    "    epoch_min_loss = float('inf')\n",
    "\n",
    "    print(f'Total samples: {total_samples}, Batch size: {batch_size}, Maximum iterations: {max_iterations}')\n",
    "\n",
    "    avg_every = 25\n",
    "    train_loss = train_one_epoch(model, train_data_loader, criterion, optimizer, device, epoch, avg_every)\n",
    "    val_loss = evaluate(model, val_data_loader, criterion, device)\n",
    "    print(f'VALIDATION LOSS FOR EPOCH {epoch + 1}: {val_loss:.4f}')\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    print(f'Epoch {epoch+1} total time: {epoch_end - epoch_start}')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "\n",
    "        save_name = f'best_loss_model1.pt'\n",
    "        torch.save(model.state_dict(), save_name)\n",
    "        print(f'**********NEW BEST MODEL SAVED @ VAL: {best_val_loss}**********')\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "training_end = time.time()\n",
    "print(f'Total training time: {training_end - training_start}')\n",
    "\n",
    "plot_and_save(train_losses, val_losses, learning_rates, max_min_loss_diffs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T19:40:55.732690Z",
     "end_time": "2023-04-26T19:40:55.732961Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "    # epoch_train_start = time.time()\n",
    "    # for i, (images, captions) in enumerate(train_data_loader):\n",
    "    #     images = images.to(device)\n",
    "    #     captions_input = captions[:, :-1].to(device)\n",
    "    #     captions_target = captions[:, 1:].to(device)\n",
    "    #\n",
    "    #     optimizer.zero_grad()\n",
    "    #     output = model(images, captions_input)\n",
    "    #\n",
    "    #     loss = criterion(output.reshape(-1, 28796), captions_target.view(-1))\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #\n",
    "    #     train_loss += loss.item()\n",
    "    #\n",
    "    #     if loss.item() > epoch_max_loss:\n",
    "    #         epoch_max_loss = loss.item()\n",
    "    #         print(f'Max loss set to: {epoch_max_loss}')\n",
    "    #     if loss.item() < epoch_min_loss:\n",
    "    #         epoch_min_loss = loss.item()\n",
    "    #         print(f'Min loss set to: {epoch_min_loss}')\n",
    "    #\n",
    "    #     if i % 50 == 0:\n",
    "    #         print(f'Epoch: {epoch+1}/{num_epochs}, Iteration: {i}, Loss: {loss.item()}')\n",
    "    #\n",
    "    # epoch_train_end = time.time()\n",
    "    # epoch_train_time = epoch_train_end - epoch_train_start\n",
    "    # print(f'Epoch {epoch+1} training time: {epoch_train_time}')\n",
    "    #\n",
    "    # epoch_max_min_diff = epoch_max_loss - epoch_min_loss\n",
    "    # if epoch + 1 != 1:\n",
    "    #     max_min_loss_diffs.append(epoch_max_min_diff)\n",
    "    # print(f'Difference between max and min loss in epoch {epoch+1}: {epoch_max_min_diff}')\n",
    "    #\n",
    "    # train_loss /= len(train_data_loader)\n",
    "    #\n",
    "    # model.eval()\n",
    "    # val_loss = 0\n",
    "\n",
    "#     epoch_val_start = time.time()\n",
    "#     with torch.no_grad():\n",
    "#         for images, captions in val_data_loader:\n",
    "#             images = images.to(device)\n",
    "#             captions_input = captions[:, :-1].to(device)\n",
    "#             captions_target = captions[:, 1:].to(device)\n",
    "#\n",
    "#             output = model(images, captions_input)\n",
    "#             loss = criterion(output.reshape(-1, 28796), captions_target.view(-1))\n",
    "#\n",
    "#             val_loss += loss.item()\n",
    "#\n",
    "#     epoch_val_end = time.time()\n",
    "#     epoch_val_time = epoch_val_end - epoch_val_start\n",
    "#     print(f'Epoch {epoch+1} validation time: {epoch_val_time}')\n",
    "#\n",
    "#     epoch_end = time.time()\n",
    "#     epoch_time = epoch_end - epoch_start\n",
    "#     print(f'Epoch {epoch+1} total time: {epoch_time}')\n",
    "#\n",
    "#     val_loss /= len(val_data_loader)\n",
    "#     print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "#     train_losses.append(train_loss)\n",
    "#     val_losses.append(val_loss)\n",
    "#     learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "#\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#\n",
    "#         if os.path.exists(save_name):\n",
    "#             os.remove(save_name)\n",
    "#\n",
    "#         save_name = f'best_loss_model_{epoch}.pth'\n",
    "#         torch.save(model.state_dict(), save_name)\n",
    "#\n",
    "#     scheduler.step()\n",
    "#\n",
    "# training_end = time.time()\n",
    "# training_time = training_end - training_start\n",
    "# print(f'Total training time: {training_time}')\n",
    "#\n",
    "# plot_and_save(train_losses, val_losses, learning_rates, max_min_loss_diffs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
